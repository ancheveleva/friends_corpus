{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62jXXQaiW4li"
   },
   "source": [
    "# FRIENDS CORPUS\n",
    "\n",
    "Привет! Это протоверсия корпуса по сериалу «Друзья». Чтобы воспользоваться поиском, нажми **Run all**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9-cnZ4apgP3H"
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect('friends_corpus_new.db')\n",
    "cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "c2Qt8-6h9Wqc"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q97LCnBChD2l"
   },
   "source": [
    "любая форма --> любая форма\n",
    "\n",
    "1. лемматизировать инпут\n",
    "2. найти айди леммы\n",
    "3. найти айди токенов\n",
    "4. найти предложения с токенами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Saq9n0xPhwXa"
   },
   "outputs": [],
   "source": [
    "def get_lemma_ids(lemma):\n",
    "    cursor.execute('''SELECT lemma_id FROM Lemmas\n",
    "    WHERE lemma = ?''', (lemma,))\n",
    "    return [i[0] for i in cursor.fetchall()]\n",
    "\n",
    "def get_token_ids(lemma_ids):\n",
    "    get_tokens_query = '''\n",
    "    SELECT token_id FROM Tokens\n",
    "    WHERE lemma_id = ?\n",
    "    '''\n",
    "    token_ids = []\n",
    "    for lemma_id in lemma_ids:\n",
    "        cursor.execute(get_tokens_query, (lemma_id,))\n",
    "        token_ids.extend([i[0] for i in cursor.fetchall()])\n",
    "    return token_ids\n",
    "\n",
    "def get_sentence_ids(token_ids):\n",
    "    get_sent_query = '''\n",
    "    SELECT sentence_id FROM Match\n",
    "    WHERE token_id = ?\n",
    "    '''\n",
    "    sentence_ids = []\n",
    "    for token_id in token_ids:\n",
    "        cursor.execute(get_sent_query, (token_id,))\n",
    "        sentence_ids.extend([i[0] for i in cursor.fetchall()])\n",
    "    return sentence_ids\n",
    "\n",
    "def token_to_tokens(token, model=model):\n",
    "    lemma = model(token)[0].lemma_\n",
    "    if lemma.startswith('-') and lemma.endswith('-'):\n",
    "        lemma = token\n",
    "    return get_sentence_ids(get_token_ids(get_lemma_ids(lemma)))\n",
    "\n",
    "#token_to_tokens('you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z74I8t_f6i1n"
   },
   "outputs": [],
   "source": [
    "#model('you')[0].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb5wGB3wpPcq"
   },
   "source": [
    "любая форма --> эта же форма\n",
    "\n",
    "1. найти айди токена (если нет, то лемматизировать и как I)\n",
    "2. найти айди предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UJsFm2fhmrtP"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_token_id(token):\n",
    "    get_token_query = '''\n",
    "    SELECT token_id FROM Tokens\n",
    "    WHERE token = ?\n",
    "    '''\n",
    "    cursor.execute(get_token_query, (token,))\n",
    "    return [i[0] for i in cursor.fetchall()]\n",
    "\n",
    "def token_to_token(token):\n",
    "    return get_sentence_ids(get_token_id(token))\n",
    "    \n",
    "#token_to_token('snows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90GxvVCosuip"
   },
   "source": [
    "лемма+пос --> любая форма\n",
    "\n",
    "1. найти лемма айди\n",
    "2. найти айди токенов\n",
    "3. найти предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zAedeus3p8q5"
   },
   "outputs": [],
   "source": [
    "def get_pos_lemma_ids(lemma, pos):\n",
    "\n",
    "    get_pos_lemma_query = '''\n",
    "    SELECT lemma_id FROM Lemmas\n",
    "    WHERE (lemma = ?) AND (pos = ?)\n",
    "    '''\n",
    "    cursor.execute(get_pos_lemma_query, (lemma, pos,))\n",
    "    try:\n",
    "        return cursor.fetchall()[0]\n",
    "    except IndexError:\n",
    "        return []\n",
    "\n",
    "def lemma_pos_to_token(token, pos):\n",
    "    return get_sentence_ids(get_token_ids(get_pos_lemma_ids(token, pos)))\n",
    "\n",
    "#lemma_pos_to_token('like', 'VERB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Elnbs3PKvTIn"
   },
   "source": [
    "пос -- > токены\n",
    "1. найти леммы айди\n",
    "2. найти токены айди\n",
    "3. найти предложения айди"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2BGV_wr8xylH"
   },
   "outputs": [],
   "source": [
    "def pos_to_tokens(pos):\n",
    "    query = '''\n",
    "    SELECT sentence_id FROM Match\n",
    "    JOIN Tokens ON Match.token_id = Tokens.token_id\n",
    "    JOIN Lemmas ON Tokens.lemma_id = Lemmas.lemma_id\n",
    "    WHERE Lemmas.pos = ?\n",
    "    '''\n",
    "    cursor.execute(query, (pos,))\n",
    "    return [i[0] for i in cursor.fetchall()]\n",
    "\n",
    "#pos_to_tokens('PART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ccrj8clRMwt_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "\n",
    "def execute_query(initial_query):\n",
    "    queries = initial_query.split()\n",
    "    sentence_ids = {}\n",
    "    patterns = []\n",
    "\n",
    "    for query in queries:\n",
    "        if '+' in query:\n",
    "            lemma, pos = query.split('+')\n",
    "            new = set(lemma_pos_to_token(lemma, pos))\n",
    "            patterns.append(re.compile(f\"[a-zA-Z'\\-_]+\\+{lemma}\\+{pos}\"))\n",
    "        elif query in tags:\n",
    "            new = set(pos_to_tokens(query))\n",
    "            patterns.append(re.compile(f\"[a-zA-Z'\\-_]+\\+[a-zA-Z'\\-_]+\\+{query}\"))\n",
    "        elif query.startswith('\"') or query.startswith(\"'\"):\n",
    "            new = set(token_to_token(query[1:-1]))\n",
    "            patterns.append(re.compile(f\"{query[1:-1]}\\+[a-zA-Z'\\-_]+\\+[A-Z]+\"))\n",
    "        else:\n",
    "            new = set(token_to_tokens(query))\n",
    "            patterns.append(re.compile(f\"[a-zA-Z'\\-_]+\\+{query}\\+[A-Z]+\"))\n",
    "        # print(bool(new))\n",
    "        sentence_ids = sentence_ids.intersection(new) if sentence_ids else new\n",
    "    return sentence_ids, patterns\n",
    "\n",
    "#sents, patterns = execute_query(\"you 'need' ADV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TARrl5PAsLXH"
   },
   "outputs": [],
   "source": [
    "def get_annotated(sents_id):    \n",
    "    get_annotated_query = f'''\n",
    "    SELECT sentence_id, annotated FROM Sentences\n",
    "    WHERE sentence_id IN ({('?, ' * len(sents_id))[:-2]})\n",
    "    '''\n",
    "    cursor.execute(get_annotated_query, tuple(sents_id))\n",
    "    return [i for i in cursor.fetchall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eDLypm3tvTbT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_valid(observed, last, sentence):\n",
    "    if not last:\n",
    "        last = observed.span()[-1]\n",
    "        return True, last\n",
    "    beg, end = observed.span()\n",
    "    return beg == last + 1, end\n",
    "\n",
    "def is_sentence_valid(sentence, patterns):\n",
    "\n",
    "    matched = []\n",
    "    for pattern in patterns:\n",
    "        matched.append([x for x in re.finditer(pattern, sentence)])\n",
    "\n",
    "    last = 0\n",
    "    correct = True\n",
    "\n",
    "    for match in matched:\n",
    "        if len(match) == 1:\n",
    "            observed = match[0]\n",
    "            valid, last = is_valid(observed, last, sentence)\n",
    "            if not valid:\n",
    "                correct = False\n",
    "                break\n",
    "        else:\n",
    "            valid_is_found = False\n",
    "            for observed in match:\n",
    "                if not valid_is_found:\n",
    "                    result = is_valid(observed, last, sentence)\n",
    "                    if result[0] == True:\n",
    "                        last = result[-1]\n",
    "                        valid_is_found = True\n",
    "            if not valid_is_found:\n",
    "                correct = False\n",
    "                break\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8iUIv-1SB5nl"
   },
   "outputs": [],
   "source": [
    "#is_sentence_valid('oh+oh+INTJ hey+hey+INTJ but+but+CCONJ ,+,+PUNCT before+before+ADP you+you+PRON guys+guy+NOUN do+do+VERB that+that+SCONJ i+I+PRON need+need+VERB to+to+PART talk+talk+VERB to+to+ADP you+you+PRON ,+,+PUNCT and+and+CCONJ ross+Ross+PROPN ,+,+PUNCT i+I+PRON need+need+VERB to+to+PART talk+talk+VERB to+to+ADP you+you+PRON .+.+PUNCT', patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IYQsGV0jVrlD"
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ScXgW4LV5uGZ"
   },
   "outputs": [],
   "source": [
    "def find(query):\n",
    "    sentences_ids, patterns = execute_query(query)\n",
    "    annotated = get_annotated(sentences_ids)\n",
    "    valid = [sentence[0] for sentence in annotated if is_sentence_valid(sentence[-1], patterns)]\n",
    "\n",
    "    get_everything = f'''\n",
    "    SELECT sentence, character, episode, season, scriptwriters\n",
    "    FROM Sentences\n",
    "    JOIN Meta ON Sentences.sentence_id = Meta.sentence_id\n",
    "    WHERE Sentences.sentence_id IN ({('?, ' * len(valid))[:-2]})\n",
    "    '''\n",
    "\n",
    "    cursor.execute(get_everything, valid)\n",
    "\n",
    "    result = cursor.fetchall()\n",
    "    with open(\"./results.csv\", 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow([\"sentence\", \"character\", \"episode\", \"season\", \"scriptwriters\"])\n",
    "        for sent in result:\n",
    "            wr.writerow(sent)\n",
    "    #return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "elutrix8FViM"
   },
   "outputs": [],
   "source": [
    "find('do+AUX do+VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO5sK7TCtDpR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
